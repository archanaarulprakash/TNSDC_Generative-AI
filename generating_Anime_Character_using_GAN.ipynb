{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiAVZyomZYhg",
        "outputId": "42d8d73d-a73e-4a24-cd1c-5838ddbe0c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading anime-images, 167528348 bytes compressed\n",
            "[=======================                           ] 79462400 bytes downloaded"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'anime-images:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2931086%2F5048758%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240331%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240331T120037Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2c2ba551093ac52fddc4a24fa98e7bcc6d5052b13bd7ad15e61a85f27412c7cea8ac2613605dd670360152b87ae8c0864311dd9481fff480b0dfff5f09619bfbce6674a2ef1310ea02b8f8eecad278deb83df521bc2d7586248f8eba90b74d1bd5451867a20387edd8e1c7d130f2b72ee3017f3ee3da45a53742ebc5f2745d82c5be7a1df60a03274a2b4f3a2294a849ed338fa5dc2d5c467caab14fd4fe00e8db155f16a0d001ee15ce829249ae1fe24298a007c7c1757ec385532ec51a49d451cda67b0f10adf2a80a55a4e611564fe737739e40c7412d554763a31f0fd29a60bf816665ef95ea14f2578b103fbf3e45870512f913cce93c2afee955615f4a'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5CRk3GjZYhk"
      },
      "outputs": [],
      "source": [
        "# Styling notebook\n",
        "from IPython.core.display import HTML\n",
        "css_file= \"\"\"\n",
        "<style>\n",
        "div.input_area {\n",
        "    background-color: #F1F0F0; /* gray */\n",
        "    border-top: 2px solid #000000; /* black */\n",
        "}\n",
        "\n",
        "@import url('https://fonts.googleapis.com/css?family=Quicksand&display=swap');\n",
        " * {\n",
        "    margin: 0;\n",
        "    padding: 0;\n",
        "    box-sizing: border-box;\n",
        "}\n",
        "\n",
        " .alert {\n",
        "    width: 80%;\n",
        "    margin: 20px auto;\n",
        "    padding: 30px;\n",
        "    position: relative;\n",
        "    border-radius: 5px;\n",
        "    box-shadow: 0 0 15px 5px #ccc;\n",
        "}\n",
        " .close {\n",
        "    position: absolute;\n",
        "    width: 30px;\n",
        "    height: 30px;\n",
        "    opacity: 0.5;\n",
        "    border-width: 1px;\n",
        "    border-style: solid;\n",
        "    border-radius: 50%;\n",
        "    right: 15px;\n",
        "    top: 25px;\n",
        "    text-align: center;\n",
        "    font-size: 1.6em;\n",
        "    cursor: pointer;\n",
        "}\n",
        " .simple-alert {\n",
        "    background-color: #aed6e5;\n",
        "    border-left: 5px solid #245b70;\n",
        "}\n",
        " .simple-alert .close {\n",
        "    border-color: #245b70;\n",
        "    color: #245b70;\n",
        "}\n",
        " .success-alert {\n",
        "    background-color: #aee5c0;\n",
        "    border-left: 5px solid #24703d;\n",
        "}\n",
        " .success-alert .close {\n",
        "    border-color: #24703d;\n",
        "    color: #24703d;\n",
        "}\n",
        " .danger-alert {\n",
        "    background-color: #e5aeae;\n",
        "    border-left: 5px solid #702424;\n",
        "}\n",
        " .danger-alert .close {\n",
        "    border-color: #702424;\n",
        "    color: #702424;\n",
        "}\n",
        " .warning-alert {\n",
        "    background-color: #ffe6a9;\n",
        "    border-left: 5px solid #a97800;\n",
        "}\n",
        " .warning-alert .close {\n",
        "    border-color: #a97800;\n",
        "    color: #a97800;\n",
        "}\n",
        "\n",
        "li {\n",
        "  list-style: none; /* Remove default bullets */\n",
        "}\n",
        "\n",
        "ul li::before {\n",
        "  content: \"â€¢\";\n",
        "  color: #dc8615;\n",
        "  font-weight: bold;\n",
        "  display: inline-block; /* Needed to add space between the bullet and the text */\n",
        "  width: 1em; /* Also needed for space (tweak if needed) */\n",
        "  margin-left: -1em; /* Also needed for space (tweak if needed) */\n",
        "  font-size:120%;\n",
        "}\n",
        "\n",
        ".number{\n",
        "    color:#dc8615;\n",
        "}\n",
        "\n",
        "mark {\n",
        " background-color:#FC9D2F;\n",
        " color:black;\n",
        " border-radius: 6px;\n",
        " padding: 2px 5px;\n",
        "}\n",
        "\n",
        ".btn {\n",
        "  background-color: gray;\n",
        "  border: none;\n",
        "  color: white !important;\n",
        "  padding: 5px 15px;\n",
        "  font-size: 18px;\n",
        "  cursor: pointer;\n",
        "}\n",
        "\n",
        "/* Darker background on mouse-over */\n",
        ".btn:hover {\n",
        "  background-color: #FC9D2F;\n",
        "}\n",
        "\n",
        "</style>\n",
        "\"\"\"\n",
        "HTML(css_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTH71gwrZYhl"
      },
      "source": [
        "# <p style=\"background:yellow ;color:red;font-family:newtimeroman;font-size:110%;text-align:center;border-radius:10px 10px;\"> Createing new faces of anime characters using Generative Adversarial Network (GAN). </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjmAHLBVZYhn"
      },
      "source": [
        "![dcgan.gif](attachment:dcgan.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T7iDIj2ZYhn"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jMDnB8yZYho"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets as torch_dataset\n",
        "from torchvision.utils import make_grid\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLef7kpAZYho"
      },
      "outputs": [],
      "source": [
        "!pip install torch-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4FnviDnZYhp"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I89ifrVEZYhp"
      },
      "source": [
        "## Loading dataset(Images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuC9FvCKZYhq"
      },
      "outputs": [],
      "source": [
        "rootPath = r'/kaggle/input/anime-images/GAN img'\n",
        "File_name = sorted(os.listdir(rootPath))[:]\n",
        "File_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb6P0pyoZYhq"
      },
      "outputs": [],
      "source": [
        "sizes = [len(os.listdir(rootPath + '/' + name)) for name in File_name]\n",
        "print('Number of images: ', sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cespIdXJZYhq"
      },
      "outputs": [],
      "source": [
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2MBNgWNZYhq"
      },
      "source": [
        "### Showing some random images and their sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REWANnVSZYhr"
      },
      "outputs": [],
      "source": [
        "def load_random_img(dir):\n",
        "    plt.figure(figsize=(2,2))\n",
        "    for P in File_name:\n",
        "        file = random.choice(os.listdir(f'{dir}/{P}'))\n",
        "        image_path = os.path.join(f'{dir}/{P}', file)\n",
        "        img=cv2.imread(image_path)\n",
        "        plt.imshow(img)\n",
        "        print('Image size is: ', img.shape)\n",
        "        plt.grid(None)\n",
        "        plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMDzu3MIZYhr"
      },
      "outputs": [],
      "source": [
        "load_random_img(rootPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtdNMassZYhr"
      },
      "outputs": [],
      "source": [
        "load_random_img(rootPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRunFhkGZYhr"
      },
      "outputs": [],
      "source": [
        "load_random_img(rootPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xek8GVvZYhr"
      },
      "source": [
        "### We Feed these images into the discriminator as real images. Once GAN is trained, the generator will produce realistic-looking anime faces, like the ones shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcJuQmYdZYhr"
      },
      "source": [
        "#### Loading and Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_t6zIJAZYhr"
      },
      "outputs": [],
      "source": [
        "# WE pass a list of transforms to be composed.\n",
        "'''\n",
        "- The anime face images are of varied sizes. First, resize them to a fixed size of 64 x 64.\n",
        "\n",
        "- Then normalize, using the mean and standard deviation of 0.5. Note that both mean & variance\n",
        "  have three values, as we are dealing with an RGB image.\n",
        "\n",
        "- The normalization maps the pixel values from the range [0, 255] to the range [-1, 1].\n",
        "  Mapping pixel values between [-1, 1] has proven useful while training GANs.\n",
        "\n",
        "- Also, convert the images to torch tensors.\n",
        "'''\n",
        "\n",
        "data_dir = r'/kaggle/input/anime-images/GAN img'\n",
        "\n",
        "data_transforms = T.Compose([\n",
        "                  T.Resize((64, 64)),\n",
        "                  T.ToTensor(),\n",
        "                  T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "\n",
        "# We load the Anime Face Dataset and apply the data_transform (resizing, normalization and converting images to tensors).\n",
        "anime_dataset = torch_dataset.ImageFolder(root=data_dir, transform=data_transforms)\n",
        "# It defines the training data loader, which combines the Anime dataset to provide an iterable over the dataset used while training.\n",
        "dataloader = DataLoader(dataset=anime_dataset, batch_size=128, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VjwyYpOZYhr"
      },
      "source": [
        "### Looking at some images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtc8pVKOZYhs"
      },
      "outputs": [],
      "source": [
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "# Plot some training images\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kWVc88QZYhs"
      },
      "source": [
        "# Weights Initialization\n",
        "### Define the weight initialization function, which is called on the generator and discriminator model layers. The function   checks if the layer passed to it is a convolution layer or the batch-normalization layer.\n",
        "\n",
        "- All the convolution-layer weights are initialized from a zero-centered normal distribution, with a standard deviation of   0.02.\n",
        "- The batch-normalization layer weights are initialized with a normal distribution, having mean 1 and a standard deviation   of 0.02. The bias is initialized with zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72ud9nHVZYhs"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJQY5zvyZYhs"
      },
      "source": [
        "### Conv layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj4nz3a1ZYhs"
      },
      "outputs": [],
      "source": [
        "def Conv(n_input, n_output, k_size=4, stride=2, padding=0, bn=False):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            n_input, n_output,\n",
        "            kernel_size=k_size,\n",
        "            stride=stride,\n",
        "            padding=padding, bias=False),\n",
        "        nn.BatchNorm2d(n_output),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Dropout(p=0.2, inplace=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdvjMmAOZYhs"
      },
      "source": [
        "### Deconv layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G01TKSDOZYhs"
      },
      "outputs": [],
      "source": [
        "def Deconv(n_input, n_output, k_size=4, stride=2, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(\n",
        "            n_input, n_output,\n",
        "            kernel_size=k_size,\n",
        "            stride=stride, padding=padding,\n",
        "            bias=False),\n",
        "        nn.BatchNorm2d(n_output),\n",
        "        nn.ReLU(inplace=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mpzapIqZYhs"
      },
      "source": [
        "# Generator Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZDO0gHYZYht"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z=100, nc=64):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            Deconv(z, nc*8, 4,1,0),\n",
        "            Deconv(nc*8, nc*4, 4,2,1),\n",
        "            Deconv(nc*4, nc*2, 4,2,1),\n",
        "            Deconv(nc*2, nc, 4,2,1),\n",
        "            nn.ConvTranspose2d(nc,3, 4,2,1,bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.net(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSxs0eboZYht"
      },
      "source": [
        "# Discriminator Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo0--MqVZYht"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, nc=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                3, nc,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            Conv(nc, nc*2, 4,2,1),\n",
        "            Conv(nc*2, nc*4, 4,2,1),\n",
        "            Conv(nc*4, nc*8, 4,2,1),\n",
        "            nn.Conv2d(nc*8, 1,4,1,0, bias=False),\n",
        "            nn.Sigmoid())\n",
        "    def forward(self, input):\n",
        "        return self.net(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ3YDYv6ZYht"
      },
      "source": [
        "### Applying the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfBltQobZYht"
      },
      "outputs": [],
      "source": [
        "#device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "dis_model = Discriminator()\n",
        "gen_model = Generator()\n",
        "\n",
        "gen_model.apply(weights_init)\n",
        "dis_model.apply(weights_init)\n",
        "dis_model.to(device)\n",
        "gen_model.to(device)\n",
        "print('init model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeB8IxOJZYht"
      },
      "outputs": [],
      "source": [
        "gen_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3miA52tZYht"
      },
      "outputs": [],
      "source": [
        "dis_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S03zEtJUZYht"
      },
      "outputs": [],
      "source": [
        "summary(gen_model, (100,1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9CD9PLcZYht"
      },
      "outputs": [],
      "source": [
        "summary(dis_model, (3, 64, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL9EFnjMZYhu"
      },
      "source": [
        "## Loss Function & Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM-mbC6SZYh2"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Loss Function\n",
        "criterion = nn.BCELoss()\n",
        "# Optimizer\n",
        "optim_D = optim.Adam(dis_model.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optim_G = optim.Adam(gen_model.parameters(), lr=lr, betas=(beta1, 0.999))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjwTSl9iZYh3"
      },
      "source": [
        "# Train Mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q2fMJUJZYh3"
      },
      "outputs": [],
      "source": [
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 1\n",
        "epoch_nb = 30\n",
        "fixed_noise = torch.randn(32, 100, 1,1, device=device)\n",
        "D_x = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_eQVvFlZYh3"
      },
      "source": [
        "# Training the Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UgwU5rRFZYh3"
      },
      "outputs": [],
      "source": [
        "from torch.distributions.uniform import Uniform\n",
        "\n",
        "for epoch in range(epoch_nb):\n",
        "    for i, data in enumerate(dataloader):\n",
        "        # Train Discriminator\n",
        "        ## Train with real image\n",
        "        dis_model.zero_grad()\n",
        "        real_img = data[0].to(device)\n",
        "        bz = real_img.size(0)\n",
        "\n",
        "        #  label smoothing\n",
        "        label = Uniform(0.9, 1.0).sample((bz,)).to(device)\n",
        "#         label = torch.full((bz,), real_label, device=device, dtype=torch.float)\n",
        "\n",
        "        output = dis_model(real_img).view(-1)\n",
        "        error_real = criterion(output, label)\n",
        "        error_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with fake image\n",
        "        noise = torch.randn(bz, 100, 1,1, device=device)\n",
        "        fake_img = gen_model(noise)\n",
        "        label = Uniform(0., 0.05).sample((bz,)).to(device)\n",
        "\n",
        "        output = dis_model(fake_img.detach()).view(-1)\n",
        "        error_fake = criterion(output, label)\n",
        "        error_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        error_D = error_real + error_fake\n",
        "#         error_D.backward()\n",
        "        optim_D.step()\n",
        "\n",
        "        ## Train Generator\n",
        "        gen_model.zero_grad()\n",
        "#         noise = torch.randn(bz, 100, 1,1, device=device)\n",
        "#         fake_img = gen_model(noise)\n",
        "        label = Uniform(0.95, 1.0).sample((bz,)).to(device)\n",
        "        output = dis_model(fake_img).view(-1)\n",
        "        error_G = criterion(output, label)\n",
        "        error_G.backward()\n",
        "        optim_G.step()\n",
        "        D_G_z2 = output.mean().item()\n",
        "\n",
        "        if i % 300 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, epoch_nb, i, len(dataloader),\n",
        "                     error_D.item(), error_G.item(), D_x, D_G_z1, D_G_z2))\n",
        "        if epoch > 1:\n",
        "            if (iters % 1000 == 0) or ((epoch == epoch_nb-1) and (i == len(dataloader)-1)):\n",
        "                with torch.no_grad():\n",
        "                    fake_img = gen_model(fixed_noise).detach().cpu()\n",
        "                fake_img = make_grid(fake_img, padding=2, normalize=True)\n",
        "                img_list.append(fake_img)\n",
        "                plt.figure(figsize=(10,10))\n",
        "                plt.imshow(img_list[-1].permute(1,2,0))\n",
        "                plt.show()\n",
        "\n",
        "        iters += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc0TzAtvY7BE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06AFXj87ZYh3"
      },
      "source": [
        "## Animated show of generated images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YKfrhtN_ZYh3"
      },
      "outputs": [],
      "source": [
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "imgs = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "img_animation = animation.ArtistAnimation(fig, imgs, interval=1000, repeat_delay=100, blit=True)\n",
        "HTML(img_animation.to_jshtml())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}